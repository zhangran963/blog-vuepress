```py
from mxnet import autograd,nd

# 创建X
x = nd.arange(4).reshape((4,1))

# 先调用attach_grad函数来申请存储梯度所需要的内存
x.attach_grad()

# 为了减少计算和内存开销，默认条件下MXNet不会记录用于求梯度的计算。我们需要调用record函数来要求MXNet记录与求梯度有关的计算
with autograd.record():
    y = 2*nd.dot(x.T,x)


# y是标量, 可以通过调用backward函数自动求梯度
# y不是标量，MXNet将默认先对y中元素求和得到新的变量，再求该变量有关x的梯度
y.backward()

# 验证
# assert (x.grad - 4 * x).norm().asscalar() == 0

# 结果
print(x.grad)


# 查看是否开始"训练模式", 另一种是"预测模式"
autograd.is_training()
```